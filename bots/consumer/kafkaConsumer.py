
"""
this script consumes the job from the producer and invokes the pipeline of the job_scheduler

"""

import platform
from kafka import  KafkaConsumer
from dotenv import load_dotenv, dotenv_values
import logging
import json
import time
import os
#from bots.utils.pipeline_construction_caller import  execute_reconstruction_pipeline

from utils.model_helper import InputParametersPoint


logger = logging.getLogger()
logger.setLevel(logging.INFO)
log=logging.getLogger("discord")

load_dotenv(dotenv_path='./../.env')


consumer: KafkaConsumer = KafkaConsumer(
          bootstrap_servers=['growing-krill-8066-eu1-kafka.upstash.io:9092' ],
  sasl_mechanism='SCRAM-SHA-256',
  security_protocol='SASL_SSL',
  sasl_plain_username= os.getenv("SASL_PLAIN_USERNAME"),
  sasl_plain_password=os.getenv("SASL_PLAIN_PASSWORD"),
        auto_offset_reset='earliest',
        consumer_timeout_ms=1000
    )
    
def kafka_consume_message_jobInput(topic: str = 'bacalhau_compute_job', username: str = "" ):
    """
    This consumes the bot input command messages and parameters and then parses and runs the corresponding job on the pipeline
    
    topic: its the corresponding topic from bacalhau to consume from.
    username: is the discord username , which is added as the unique identifier for the topic. 
    """
    time.sleep(5)
    consumer.subscribe(topic) 
    logger.info('now fetching the response of username provided by the user')
    ## only consider last message for consuming for bacalhau 
    
    for message in consumer:
        if(message[username].key == username):
            current_message_offset = message[username].value
            break
        
    consumer.seek(partition=topic, offset=current_message_offset)
    
    value  = consumer.poll(topic)
    
    ## now converting the given parameters to string outputs
    params = []
    for i in value.decode('utf-8').split(","):
        params.append(i)
    
    ## now running the container jobs with the parameters
    jobParameter = InputParametersPoint() 
    
    jobParameter.coordX = params[0]
    jobParameter.coordY = params[1]
    jobParameter.dockerimage = params[2]
    jobParameter.filename_shp = params[3]
    jobParameter.ipfs_image = params[4]
    jobParameter.username = params[5]    
    #createJobBacalauPoint(jobParameter)
    
    #return parameters

def kafka_consume_list_jobs(username: str):
    topic = 'bacalhau_compute_job'
    consumer.subscribe(topic) 
    logger.info('fetching the current jobs generated by the user')
    current_message_offset = 0
    for message in consumer:
        if(message[username].key == username):
            current_message_offset = message[username].value
            break
    
    consumer.seek(partition=topic, offset=current_message_offset)
    
    parameters = json.loads(consumer.poll(topic))
    
    return parameters    